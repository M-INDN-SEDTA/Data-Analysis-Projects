{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3a150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded with shape: (44898, 12)\n",
      "Inserted batch 1 to 1000\n",
      "Inserted batch 1001 to 2000\n",
      "Inserted batch 2001 to 3000\n",
      "Inserted batch 3001 to 4000\n",
      "Inserted batch 4001 to 5000\n",
      "Inserted batch 5001 to 6000\n",
      "Inserted batch 6001 to 7000\n",
      "Inserted batch 7001 to 8000\n",
      "Inserted batch 8001 to 9000\n",
      "Inserted batch 9001 to 10000\n",
      "Inserted batch 10001 to 11000\n",
      "Inserted batch 11001 to 12000\n",
      "Inserted batch 12001 to 13000\n",
      "Inserted batch 13001 to 14000\n",
      "Inserted batch 14001 to 15000\n",
      "Inserted batch 15001 to 16000\n",
      "Inserted batch 16001 to 17000\n",
      "Inserted batch 17001 to 18000\n",
      "Inserted batch 18001 to 19000\n",
      "Inserted batch 19001 to 20000\n",
      "Inserted batch 20001 to 21000\n",
      "Inserted batch 21001 to 22000\n",
      "Inserted batch 22001 to 23000\n",
      "Inserted batch 23001 to 24000\n",
      "Inserted batch 24001 to 25000\n",
      "Inserted batch 25001 to 26000\n",
      "Inserted batch 26001 to 27000\n",
      "Inserted batch 27001 to 28000\n",
      "Inserted batch 28001 to 29000\n",
      "Inserted batch 29001 to 30000\n",
      "Inserted batch 30001 to 31000\n",
      "Inserted batch 31001 to 32000\n",
      "Inserted batch 32001 to 33000\n",
      "Inserted batch 33001 to 34000\n",
      "Inserted batch 34001 to 35000\n",
      "Inserted batch 35001 to 36000\n",
      "Inserted batch 36001 to 37000\n",
      "Inserted batch 37001 to 38000\n",
      "Inserted batch 38001 to 39000\n",
      "Inserted batch 39001 to 40000\n",
      "Inserted batch 40001 to 41000\n",
      "Inserted batch 41001 to 42000\n",
      "Inserted batch 42001 to 43000\n",
      "Inserted batch 43001 to 44000\n",
      "Inserted batch 44001 to 44898\n",
      "All batches inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 1. Load .env credentials\n",
    "load_dotenv(override=True)\n",
    "\n",
    "username = os.getenv(\"DB_USERNAME\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "host = \"localhost\"\n",
    "port = 3306\n",
    "database = \"true_and_fake_news_detection_db\"\n",
    "\n",
    "# 2. Load cleaned CSV file\n",
    "csv_path = \"./datasets/true_and_fake_news_clean_with_features.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"CSV loaded with shape:\", df.shape)\n",
    "\n",
    "# 3. Connect to MySQL\n",
    "conn = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    database=database,\n",
    "    port=port\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 4. Create table if not exists\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS news_articles (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    text LONGTEXT,\n",
    "    subject VARCHAR(255),\n",
    "    date DATE,\n",
    "    label INT,\n",
    "    clean_text LONGTEXT,\n",
    "    label_str VARCHAR(10),\n",
    "    word_count INT,\n",
    "    char_count INT,\n",
    "    avg_word_length FLOAT,\n",
    "    sentence_count INT,\n",
    "    polarity FLOAT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# 5. Prepare insert statement\n",
    "sql = \"\"\"\n",
    "INSERT IGNORE INTO news_articles (\n",
    "    title, text, subject, date, label,\n",
    "    clean_text, label_str, word_count, char_count,\n",
    "    avg_word_length, sentence_count, polarity\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# 6. Preprocess DataFrame\n",
    "# Replace NaN with None\n",
    "df = df.replace({pd.NA: None, pd.NaT: None})\n",
    "df = df.where(pd.notnull(df), None)\n",
    "\n",
    "# Clean whitespace\n",
    "df[\"date\"] = df[\"date\"].str.strip()\n",
    "# Now convert properly\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%B %d, %Y\", errors=\"coerce\").dt.date\n",
    "\n",
    "# 7. Convert DataFrame rows to list of tuples (handling NaN/NaT)\n",
    "values = [tuple(None if pd.isna(x) else x for x in row)\n",
    "          for row in df[[\n",
    "              \"title\", \"text\", \"subject\", \"date\", \"label\",\n",
    "              \"clean_text\", \"label_str\", \"word_count\", \"char_count\",\n",
    "              \"avg_word_length\", \"sentence_count\", \"polarity\"\n",
    "          ]].to_numpy()]\n",
    "\n",
    "# 7. Insert rows in batches\n",
    "batch_size = 1000  # due to large dataset we cant insert all at once\n",
    "for i in range(0, len(values), batch_size):\n",
    "    batch = values[i:i+batch_size]\n",
    "    cursor.executemany(sql, batch)\n",
    "    conn.commit()\n",
    "    print(f\"Inserted batch {i+1} to {i+len(batch)}\")\n",
    "\n",
    "# 8. Close cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"All batches inserted successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
